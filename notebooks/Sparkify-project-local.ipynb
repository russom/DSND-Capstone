{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Project Workspace\n",
    "This workspace contains a tiny subset (128MB) of the full dataset available (12GB). Feel free to use this workspace to build your project, or to explore a smaller subset with Spark before deploying your cluster on the cloud. Instructions for setting up your Spark cluster is included in the last lesson of the Extracurricular Spark Course content.\n",
    "\n",
    "You can follow the steps below to guide your data analysis and model building portion of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "# from pyspark.sql import Window\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import math \n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify Project\") \\\n",
    "    .config(\"spark.driver.bindAddress\",\"localhost\")\\\n",
    "    .config(\"spark.ui.port\",\"4050\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check config\n",
    "# spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check session\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\n",
    "In this workspace, the mini-dataset file is `mini_sparkify_event_data.json`. Load and clean the dataset, checking for invalid or missing data - for example, records without userids or sessionids. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Mini dataset\n",
    "path = \"../data/mini_sparkify_event_data.json\"\n",
    "# Full dataset\n",
    "# path =\"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "df_user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: long (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check schema\n",
    "df_user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Describe\n",
    "# user_log.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 1 row\n",
    "# user_log.show(n=1)\n",
    "df_user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(286500, 18)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check num of rows, columns\n",
    "df_user_log.count(), len(df_user_log.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can delete eventual NaN and missing users:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Drop NaN\n",
    "df_user_log_valid = df_user_log.dropna(how = \"any\", subset = [\"userId\", \"sessionId\"])\n",
    "# Drop empty users\n",
    "df_user_log_valid = df_user_log_valid.filter(df_user_log_valid[\"userId\"] != \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_log_valid.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "When you're working with the full dataset, perform EDA by loading a small subset of the data and doing basic manipulations within Spark. In this workspace, you are already provided a small subset of data you can explore.\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Once you've done some preliminary analysis, create a column `Churn` to use as the label for your model. I suggest using the `Cancellation Confirmation` events to define your churn, which happen for both paid and free users. As a bonus task, you can also look into the `Downgrade` events.\n",
    "\n",
    "### Explore Data\n",
    "Once you've defined churn, perform some exploratory data analysis to observe the behavior for users who stayed vs users who churned. You can start by exploring aggregates on these two groups of users, observing how much of a specific action they experienced per a certain time unit or number of songs played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|level|\n",
      "+-----+\n",
      "| free|\n",
      "| paid|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check available levels\n",
    "df_user_log_valid.select(\"level\").dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|               About|\n",
      "|          Add Friend|\n",
      "|     Add to Playlist|\n",
      "|              Cancel|\n",
      "|Cancellation Conf...|\n",
      "|           Downgrade|\n",
      "|               Error|\n",
      "|                Help|\n",
      "|                Home|\n",
      "|              Logout|\n",
      "|            NextSong|\n",
      "|         Roll Advert|\n",
      "|       Save Settings|\n",
      "|            Settings|\n",
      "|    Submit Downgrade|\n",
      "|      Submit Upgrade|\n",
      "|         Thumbs Down|\n",
      "|           Thumbs Up|\n",
      "|             Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check available pages\n",
    "df_user_log_valid.select(\"page\").dropDuplicates().sort(\"page\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defining a churn variable based on the Cancellation Confirmation page\n",
    "cancellation = F.udf(lambda x: 1 if x == \"Cancellation Confirmation\" else 0, IntegerType())  \n",
    "df_user_log_valid = df_user_log_valid.withColumn(\"churn\", cancellation(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sub_dwg variable based on the Submit Downgrade page\n",
    "submit_dwg = F.udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())\n",
    "df_user_log_valid = df_user_log_valid.withColumn(\"sub_dwg\", submit_dwg(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting UNIX time from miliseconds to seconds\n",
    "df_user_log_valid = df_user_log_valid.withColumn(\"ts\", df_user_log_valid.ts/1000)                        \n",
    "df_user_log_valid = df_user_log_valid.withColumn(\"registration\", df_user_log_valid.registration/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a last_ts variable showing the timestamp of the last entry \n",
    "# (Can be used to filter with time, will be used to calculate total duration of the permanence)\n",
    "window = Window.partitionBy(\"userId\")\n",
    "df_user_log_valid= df_user_log_valid.withColumn(\"last_ts\", F.max('ts').over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a perm_days variable showing the (rounded) number of days a user has spent with the service so far\n",
    "df_user_log_valid = df_user_log_valid.withColumn(\"perm_days\", \\\n",
    "                                                 F.round((df_user_log_valid.last_ts - df_user_log_valid.registration)\\\n",
    "                                                 /(3600*24)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can remove from the dataframe users that have less than a day of permanence\n",
    "df_user_log_valid = df_user_log_valid.filter(df_user_log_valid.perm_days > 0)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Sleeping With Sirens', auth='Logged In', firstName='Darianna', gender='F', itemInSession=0, lastName='Carpenter', length=202.97098, level='free', location='Bridgeport-Stamford-Norwalk, CT', method='PUT', page='NextSong', registration=1538016340.0, sessionId=31, song='Captain Tyin Knots VS Mr Walkway (No Way)', status=200, ts=1539003534.0, userAgent='\"Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) AppleWebKit/537.51.2 (KHTML, like Gecko) Version/7.0 Mobile/11D257 Safari/9537.53\"', userId='100010', churn=0, sub_dwg=0, last_ts=1542823952.0, perm_days=56.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check columns\n",
    "df_user_log_valid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "278148"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user_log_valid.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a list of users that cancelled vs. users that stayed\n",
    "users_cancelled = df_user_log_valid.select([\"userId\"]).where(df_user_log_valid.churn == 1).dropDuplicates().collect()\n",
    "users_staying = df_user_log_valid.select([\"userId\"]).where(df_user_log_valid.churn == 0).dropDuplicates().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 224)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(users_cancelled), len(users_staying)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build two datasets, with the users that left vs. the users that stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select a subset of the dataframe with the users that left\n",
    "users_cancelled_list = [us_c[0] for us_c in users_cancelled]\n",
    "df_user_log_cancelled = df_user_log_valid.filter(df_user_log_valid.userId.isin(users_cancelled_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select a subset of the dataframe with the users that stayed\n",
    "users_staying_list = [us_s[0] for us_s in users_staying]\n",
    "df_user_log_staying = df_user_log_valid.filter(df_user_log_valid.userId.isin(users_staying_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44864, 278148)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of the datasets\n",
    "df_user_log_cancelled.count(), df_user_log_staying.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can further refine, building datasets focusing on the last week of activity for the users, so to see wether or not there's a change for the users that churn as they approach the moment they would leave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select last week of users that cancelled\n",
    "df_user_log_cancelled_lweek = df_user_log_cancelled.filter((df_user_log_cancelled.last_ts - df_user_log_valid.ts) < \\\n",
    "                                                      (3600*24*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select last week of users that stay\n",
    "df_user_log_staying_lweek = df_user_log_staying.filter((df_user_log_staying.last_ts - df_user_log_valid.ts) < \\\n",
    "                                                      (3600*24*7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16594, 59910)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the size of the datasets\n",
    "df_user_log_cancelled_lweek.count(), df_user_log_staying_lweek.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check what happens to the first userID leaving\n",
    "# user_canc_0_lifecycle = df_user_log_valid.select([\"userId\", \"firstname\", \"page\", \"level\", \"ts\"]).\\\n",
    "#     where(df_user_log_valid.userId == users_cancelled[12][0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(user_canc_0_lifecycle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The beginning...\n",
    "# user_canc_0_lifecycle[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ... the end\n",
    "# user_canc_0_lifecycle[400:450]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a variable of interest we can consider the amount of time spent with the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How much time did he/she spend with the service:\n",
    "# print('Example of a user leaving the service:')\n",
    "# print('Date when joining: ', datetime.fromtimestamp(user_canc_0_lifecycle[0][4]/1000))\n",
    "# print('Date when leaving: ', datetime.fromtimestamp(user_canc_0_lifecycle[-1][4]/1000))\n",
    "      \n",
    "# # Quantify that time in days...\n",
    "# print('Interval in days: ', '{:.3f}'.format((user_canc_0_lifecycle[-1][4]/1000 - user_canc_0_lifecycle[0][4]/1000)\\\n",
    "#                                             /(3600*24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take a look at the same metric for a user that stayed with the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First userID staying\n",
    "# user_stay_0_lifecycle = user_log_valid.select([\"userId\", \"firstname\", \"page\", \"level\", \"ts\"]).\\\n",
    "#     where(user_log_valid.userId == users_staying[0][0]).collect()\n",
    "\n",
    "# # How much time did he/she spend with the service:\n",
    "# print('Example of a user staying with the service:')\n",
    "# print('Date when joining: ', datetime.fromtimestamp(user_stay_0_lifecycle[0][4]/1000))\n",
    "# print('Last date recorded: ', datetime.fromtimestamp(user_stay_0_lifecycle[-1][4]/1000))\n",
    "      \n",
    "# # Quantify that time in days...\n",
    "# print('Interval in days: ', '{:.3f}'.format((user_stay_0_lifecycle[-1][4]/1000 - user_stay_0_lifecycle[0][4]/1000)\\\n",
    "#                                             /(3600*24)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time spent/numer of songs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Check dta frames sizes\n",
    "user_log_cancelled.count(), user_log_staying.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select, for every user that cancelled, the number of songs and the time span they spent with the service\n",
    "song_time_canc = user_log_cancelled.\\\n",
    "                filter(user_log_cancelled[\"song\"] != \"\").\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(min(\"ts\"), max(\"ts\"), countDistinct(\"song\")).\\\n",
    "                collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the time difference\n",
    "delta_time_cancelled = [(song_time[2]/1000 - song_time[1]/1000)/(3600*24) for song_time in song_time_canc]\n",
    "# Get the number of songs listened per day day (on average)\n",
    "num_songs_cancelled = [song_time[3]/math.ceil((song_time[2]/1000 - song_time[1]/1000)/(3600*24)) for song_time in song_time_canc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Select, for every user that stays, the number of songs and the time span they spent with the service\n",
    "song_time_stay = user_log_staying.\\\n",
    "                filter(user_log_staying[\"song\"] != \"\").\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(min(\"ts\"), max(\"ts\"), countDistinct(\"song\")).\\\n",
    "                collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the time difference\n",
    "delta_time_staying = [(song_time[2]/1000 - song_time[1]/1000)/(3600*24) for song_time in song_time_stay]\n",
    "# Get the number of songs listened per day day (on average)\n",
    "num_songs_staying = [song_time[3]/math.ceil((song_time[2]/1000 - song_time[1]/1000)/(3600*24)) for song_time in song_time_stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram for time\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "fig.suptitle('Distribution of time spent by users ', fontsize=12, fontweight='bold')\n",
    "fig.set_figheight(7) \n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax1.hist(delta_time_cancelled, bins=50)\n",
    "ax1.set_xlabel('# of days spent with the service', fontsize=11)\n",
    "ax1.set_ylabel('# of users', fontsize=11)\n",
    "ax1.set_title('Cancelling Users', fontsize=11,fontweight='bold')\n",
    "\n",
    "ax2.hist(delta_time_staying, bins=50)\n",
    "ax2.set_xlabel('# of days spent with the service', fontsize=11)\n",
    "ax2.set_ylabel('# of users', fontsize=11)\n",
    "ax2.set_title('Staying Users', fontsize=11,fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for time spent with the service for users that cancelled:')\n",
    "spark.createDataFrame(delta_time_cancelled, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for time spent with the service for users that stayed:')\n",
    "spark.createDataFrame(delta_time_staying, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot histogram for songs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "fig.suptitle('Distribution of songs listened/day by users', fontsize=12, fontweight='bold')\n",
    "fig.set_figheight(7) \n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax1.hist(num_songs_cancelled, bins=50)\n",
    "ax1.set_xlabel('# of songs/day', fontsize=11)\n",
    "ax1.set_ylabel('# of users', fontsize=11)\n",
    "ax1.set_title('Cancelling Users', fontsize=11,fontweight='bold')\n",
    "\n",
    "ax2.hist(num_songs_staying, bins=50)\n",
    "ax2.set_xlabel('# of songs/day', fontsize=11)\n",
    "ax2.set_ylabel('# of users', fontsize=11)\n",
    "ax2.set_title('Staying Users', fontsize=11,fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for songs/day for users that cancelled:')\n",
    "spark.createDataFrame(num_songs_cancelled, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for songs/day for users that stay:')\n",
    "spark.createDataFrame(num_songs_staying, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the quantiles for the num of songs/day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(num_songs_cancelled, FloatType()).approxQuantile(\"value\", [0.25, 0.5, 0.75], 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(num_songs_staying, FloatType()).approxQuantile(\"value\", [0.25, 0.5, 0.75], 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Roll advert/Add friend**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other maybe useful pages are thoe marked as `Roll Advert` or `Add Friend`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defining a roll_adv variable based on the Roll Advert page\n",
    "roll_adv = udf(lambda x: 1 if x == \"Roll Advert\" else 0, IntegerType())  \n",
    "\n",
    "user_log_cancelled = user_log_cancelled.withColumn(\"roll_adv\", roll_adv(\"page\"))\n",
    "user_log_staying = user_log_staying.withColumn(\"roll_adv\", roll_adv(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Defining an add_friend variable based on the Add Friend page\n",
    "add_friend = udf(lambda x: 1 if x == \"Add Friend\" else 0, IntegerType())  \n",
    "\n",
    "user_log_cancelled = user_log_cancelled.withColumn(\"add_friend\", add_friend(\"page\"))\n",
    "user_log_staying = user_log_staying.withColumn(\"add_friend\", add_friend(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_log_cancelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "user_log_staying.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Added friend event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "added_friend_canc = user_log_cancelled.\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(min(\"ts\"), max(\"ts\"), Ssum(\"add_friend\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "added_friend_event_canc = [added_friend[3]/math.ceil((added_friend[2]/1000 - added_friend[1]/1000)/(3600*24))\\\n",
    "                           for added_friend in added_friend_canc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "added_friend_stay = user_log_staying.\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(min(\"ts\"), max(\"ts\"), Ssum(\"add_friend\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "added_friend_event_stay = [added_friend[3]/math.ceil((added_friend[2]/1000 - added_friend[1]/1000)/(3600*24))\\\n",
    "                           for added_friend in added_friend_stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for added friends\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "fig.suptitle('Distribution of added friends/day by users', fontsize=12, fontweight='bold')\n",
    "fig.set_figheight(7) \n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax1.hist(added_friend_event_canc, bins=100)\n",
    "ax1.set_xlabel('# of added friends/day', fontsize=11)\n",
    "ax1.set_ylabel('# of users', fontsize=11)\n",
    "ax1.set_title('Cancelling Users', fontsize=11,fontweight='bold')\n",
    "\n",
    "ax2.hist(added_friend_event_stay, bins=100)\n",
    "ax2.set_xlabel('# of added friends/day', fontsize=11)\n",
    "ax2.set_ylabel('# of users', fontsize=11)\n",
    "ax2.set_title('Staying Users', fontsize=11,fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for added friend/day for users that cancelled:')\n",
    "spark.createDataFrame(added_friend_event_canc, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for added friend/day for users that stay:')\n",
    "spark.createDataFrame(added_friend_event_stay, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rolled advert event:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rolled_advert_canc = user_log_cancelled.\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(min(\"ts\"), max(\"ts\"), Ssum(\"roll_adv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rolled_advert_event_canc = [rolled_adv[3]/math.ceil((rolled_adv[2]/1000 - rolled_adv[1]/1000)/(3600*24))\\\n",
    "                            for rolled_adv in rolled_advert_canc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rolled_advert_stay = user_log_staying.\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(min(\"ts\"), max(\"ts\"), Ssum(\"roll_adv\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rolled_advert_event_stay = [rolled_adv[3]/math.ceil((rolled_adv[2]/1000 - rolled_adv[1]/1000)/(3600*24)) \\\n",
    "                            for rolled_adv in rolled_advert_stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram for rolled advert\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "fig.suptitle('Distribution of rolled advert event', fontsize=12, fontweight='bold')\n",
    "fig.set_figheight(7) \n",
    "fig.set_figwidth(15)\n",
    "\n",
    "ax1.hist(rolled_advert_event_canc, bins=100)\n",
    "ax1.set_xlabel('# of rolled adverts/day', fontsize=11)\n",
    "ax1.set_ylabel('# of users', fontsize=11)\n",
    "ax1.set_title('Cancelling Users', fontsize=11,fontweight='bold')\n",
    "\n",
    "ax2.hist(rolled_advert_event_stay, bins=100)\n",
    "ax2.set_xlabel('# of rolled adverts/day', fontsize=11)\n",
    "ax2.set_ylabel('# of users', fontsize=11)\n",
    "ax2.set_title('Staying Users', fontsize=11,fontweight='bold');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for rolled advert/day for users that cancelled:')\n",
    "spark.createDataFrame(rolled_advert_event_canc, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at the statistics\n",
    "print('Statistics for rolled advert/day for users that stay:')\n",
    "spark.createDataFrame(rolled_advert_event_stay, FloatType()).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downgrade plan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a dwg variable based on the the change of level from \"paid\" to \"free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a sub_dwg variable based on the Submit Downgrade page\n",
    "sub_dwg = udf(lambda x: 1 if x == \"Submit Downgrade\" else 0, IntegerType())  \n",
    "\n",
    "user_log_cancelled = user_log_cancelled.withColumn(\"sub_dwg\", sub_dwg(\"page\"))\n",
    "user_log_staying = user_log_staying.withColumn(\"sub_dwg\", sub_dwg(\"page\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_cancelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_staying.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "downgrade_canc = user_log_cancelled.select(Ssum(\"sub_dwg\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downgrade_stay = user_log_staying.select(Ssum(\"sub_dwg\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downgrade_canc[0][0], downgrade_stay[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a pay_plan variable based on the Level page\n",
    "pay_plan = udf(lambda x: 1 if x == \"paid\" else 0, IntegerType())  \n",
    "\n",
    "user_log_cancelled = user_log_cancelled.withColumn(\"pay_plan\", pay_plan(\"level\"))\n",
    "user_log_staying = user_log_staying.withColumn(\"pay_plan\", pay_plan(\"level\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_cancelled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log_staying.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_plan_canc = user_log_cancelled.\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(max(\"pay_plan\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_plan_canc_list = [pay_plan_status[1] for pay_plan_status in pay_plan_canc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_plan_stay = user_log_staying.\\\n",
    "                groupBy(\"userId\").\\\n",
    "                agg(max(\"pay_plan\")).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pay_plan_stay_list = [pay_plan_status[1] for pay_plan_status in pay_plan_stay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pay_plan_canc_list), sum(pay_plan_stay_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downgrade_canc[0][0]/sum(pay_plan_canc_list), downgrade_stay[0][0]/sum(pay_plan_stay_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# window = Window.partitionBy(\"userId\").rangeBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "# user_log_valid_2 = user_log_valid.withColumn(\"Schurn\", Ssum(\"churn\").over(window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# user_log_valid_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# user_log_valid_2.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "Once you've familiarized yourself with the data, build out the features you find promising to train your model on. To work with the full dataset, you can follow the following steps.\n",
    "- Write a script to extract the necessary features from the smaller subset of data\n",
    "- Ensure that your script is scalable, using the best practices discussed in Lesson 3\n",
    "- Try your script on the full data set, debugging your script if necessary\n",
    "\n",
    "If you are working in the classroom workspace, you can just extract features based on the small subset of data contained here. Be sure to transfer over this work to the larger dataset when you work on your Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test, and validation sets. Test out several of the machine learning methods you learned. Evaluate the accuracy of the various models, tuning parameters as necessary. Determine your winning model based on test accuracy and report results on the validation set. Since the churned users are a fairly small subset, I suggest using F1 score as the metric to optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Steps\n",
    "Clean up your code, adding comments and renaming variables to make the code easier to read and maintain. Refer to the Spark Project Overview page and Data Scientist Capstone Project Rubric to make sure you are including all components of the capstone project and meet all expectations. Remember, this includes thorough documentation in a README file in a Github repository, as well as a web app or blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
